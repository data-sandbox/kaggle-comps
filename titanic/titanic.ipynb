{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic ML competition\n",
    "\n",
    "https://www.kaggle.com/competitions/titanic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn import svm\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import importlib\n",
    "import auxiliary as aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(Path('train.csv'))\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass   \n",
       "0            1         0       3  \\\n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp   \n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1  \\\n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Definitions**\n",
    "\n",
    "- survived: 0 = No, 1 = Yes\n",
    "- pclass: \tTicket class \t1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "- sex: \tSex \t\n",
    "- Age: \tAge in years \t\n",
    "- sibsp: \t# of siblings / spouses aboard the Titanic \t\n",
    "- parch: \t# of parents / children aboard the Titanic \t\n",
    "- ticket: \tTicket number \t\n",
    "- fare: \tPassenger fare \t\n",
    "- cabin: \tCabin number \t\n",
    "- embarked: \tPort of Embarkation \tC = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "y = train['Survived']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "We'll start with just a few features for the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass    0.000000\n",
       "Sex       0.000000\n",
       "SibSp     0.000000\n",
       "Parch     0.000000\n",
       "Age       0.198653\n",
       "dtype: float64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\"]\n",
    "num_features = ['Age']\n",
    "cat_features = [f for f in features if f not in num_features]\n",
    "\n",
    "# Check fraction of nulls\n",
    "train[features].isnull().mean()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than dealing with the Age nulls now, let's choose a simpler feature set for the baseline model and address the nulls later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass    0.0\n",
       "Sex       0.0\n",
       "SibSp     0.0\n",
       "Parch     0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "num_features = []\n",
    "cat_features = [f for f in features if f not in num_features]\n",
    "\n",
    "# Check fraction of nulls\n",
    "train[features].isnull().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train[features], y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       105\n",
      "           1       0.78      0.70      0.74        74\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.79      0.78      0.78       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ohe = ColumnTransformer([\n",
    "    ('ohe_features', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
    "    ('scaled_num', StandardScaler(), num_features)\n",
    "])\n",
    "\n",
    "lr_pipe = Pipeline([('ohe', ohe),\n",
    "                    ('lr', LogisticRegression())])\n",
    "\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "predictions = lr_pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predictions using test dataset\n",
    "test = pd.read_csv(Path('test.csv'))\n",
    "predictions = lr_pipe.predict(test[features])\n",
    "\n",
    "output = pd.DataFrame(\n",
    "    {'PassengerId': test['PassengerId'], 'Survived': predictions})\n",
    "output.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvements: Include Age\n",
    "\n",
    "We'll start by including age and addressing the null values. The first approach we'll take is simply use a mean age in place of the missing values. This is not expected to be very accurate, but is a fast and simple approach that works well for numeric columsn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\"]\n",
    "num_features = ['Age']\n",
    "cat_features = [f for f in features if f not in num_features]\n",
    "\n",
    "# Check fraction of nulls\n",
    "train[features].isnull().mean()\n",
    "\n",
    "# Test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train[features], y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       105\n",
      "           1       0.78      0.73      0.76        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.79      0.80       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ColumnTransformer([\n",
    "    ('ohe_features', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
    "    ('imputer_mean', SimpleImputer(strategy='mean'), num_features)\n",
    "])\n",
    "\n",
    "# Raises \"ValueError: Specifying the columns using strings is only supported for pandas DataFrames\"\n",
    "# scaler = ColumnTransformer([\n",
    "#     ('scaled_num', StandardScaler(), num_features)\n",
    "# ], remainder='passthrough')\n",
    "\n",
    "lr_pipe = Pipeline([('features', features),\n",
    "                    ('scaler', MaxAbsScaler()),\n",
    "                    # ('scaler', StandardScaler(with_mean=False)),\n",
    "                    ('lr', LogisticRegression())])\n",
    "\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "predictions = lr_pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minor accuracy improvement over the baseline model. Note that `MaxAbsScaler()` is necessary here to bound the data so that the logistic regressor is able to converge. Because of the sparse matrices from the one-hot encoded features, `StandardScaler()` could alternatively be used as long as `with_mean=False` is passed, but this scaler results in a 3% lower accuracy score compared to `MaxAbsScaler()`.\n",
    "\n",
    "Let's now try the more sophisticated `KNNImputer` which uses k-nearest neighbors to fill missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.83       105\n",
      "           1       0.79      0.70      0.74        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.78      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select features\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\"]\n",
    "num_features = ['Age']\n",
    "cat_features = [f for f in features if f not in num_features]\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    ('ohe_features', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
    "    ('imputer_knn', KNNImputer(n_neighbors=5), num_features),\n",
    "])\n",
    "\n",
    "lr_pipe = Pipeline([('transformer', transformer),\n",
    "                    ('scaler', MaxAbsScaler()),\n",
    "                    ('lr', LogisticRegression())])\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "predictions = lr_pipe.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No difference in the results between `SimpleImputer` and `KNNImputer`, indicating that simply using the mean age for missing ages is not a bad approximation here.\n",
    "\n",
    "Just to be thorough, let's run GridSearch on the hyperparameters for `KNNImputer()`. We'll also start wrapping the model into functions for simpler calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.83       105\n",
      "           1       0.79      0.70      0.74        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.78      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same model as above to verify function wrapping is working.\n",
    "\n",
    "importlib.reload(aux)\n",
    "\n",
    "# Select features\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\"]\n",
    "num_features = ['Age']\n",
    "cat_features = [f for f in features if f not in num_features]\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    ('ohe_features', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
    "    ('imputer_knn', KNNImputer(n_neighbors=5), num_features),\n",
    "])\n",
    "\n",
    "model = aux.fit_model(X_train, y_train, transformer, LogisticRegression())\n",
    "aux.predict_model(X_test, y_test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('transformer',\n",
       "   ColumnTransformer(transformers=[('ohe_features',\n",
       "                                    OneHotEncoder(handle_unknown='ignore'),\n",
       "                                    ['Pclass', 'Sex', 'SibSp', 'Parch']),\n",
       "                                   ('imputer_knn', KNNImputer(), ['Age'])])),\n",
       "  ('scaler', MaxAbsScaler()),\n",
       "  ('est', LogisticRegression())],\n",
       " 'verbose': False,\n",
       " 'transformer': ColumnTransformer(transformers=[('ohe_features',\n",
       "                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                  ['Pclass', 'Sex', 'SibSp', 'Parch']),\n",
       "                                 ('imputer_knn', KNNImputer(), ['Age'])]),\n",
       " 'scaler': MaxAbsScaler(),\n",
       " 'est': LogisticRegression(),\n",
       " 'transformer__n_jobs': None,\n",
       " 'transformer__remainder': 'drop',\n",
       " 'transformer__sparse_threshold': 0.3,\n",
       " 'transformer__transformer_weights': None,\n",
       " 'transformer__transformers': [('ohe_features',\n",
       "   OneHotEncoder(handle_unknown='ignore'),\n",
       "   ['Pclass', 'Sex', 'SibSp', 'Parch']),\n",
       "  ('imputer_knn', KNNImputer(), ['Age'])],\n",
       " 'transformer__verbose': False,\n",
       " 'transformer__verbose_feature_names_out': True,\n",
       " 'transformer__ohe_features': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'transformer__imputer_knn': KNNImputer(),\n",
       " 'transformer__ohe_features__categories': 'auto',\n",
       " 'transformer__ohe_features__drop': None,\n",
       " 'transformer__ohe_features__dtype': numpy.float64,\n",
       " 'transformer__ohe_features__handle_unknown': 'ignore',\n",
       " 'transformer__ohe_features__max_categories': None,\n",
       " 'transformer__ohe_features__min_frequency': None,\n",
       " 'transformer__ohe_features__sparse': 'deprecated',\n",
       " 'transformer__ohe_features__sparse_output': True,\n",
       " 'transformer__imputer_knn__add_indicator': False,\n",
       " 'transformer__imputer_knn__copy': True,\n",
       " 'transformer__imputer_knn__keep_empty_features': False,\n",
       " 'transformer__imputer_knn__metric': 'nan_euclidean',\n",
       " 'transformer__imputer_knn__missing_values': nan,\n",
       " 'transformer__imputer_knn__n_neighbors': 5,\n",
       " 'transformer__imputer_knn__weights': 'uniform',\n",
       " 'scaler__copy': True,\n",
       " 'est__C': 1.0,\n",
       " 'est__class_weight': None,\n",
       " 'est__dual': False,\n",
       " 'est__fit_intercept': True,\n",
       " 'est__intercept_scaling': 1,\n",
       " 'est__l1_ratio': None,\n",
       " 'est__max_iter': 100,\n",
       " 'est__multi_class': 'auto',\n",
       " 'est__n_jobs': None,\n",
       " 'est__penalty': 'l2',\n",
       " 'est__random_state': None,\n",
       " 'est__solver': 'lbfgs',\n",
       " 'est__tol': 0.0001,\n",
       " 'est__verbose': 0,\n",
       " 'est__warm_start': False}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get parameter names to pass GridSearch\n",
    "model.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.83       105\n",
      "           1       0.79      0.70      0.74        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.78      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'transformer__imputer_knn__n_neighbors': 1,\n",
       " 'transformer__imputer_knn__weights': 'uniform'}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(aux)\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    ('ohe_features', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
    "    ('imputer_knn', KNNImputer(missing_values=np.nan), num_features),\n",
    "])\n",
    "\n",
    "param_grid = {'transformer__imputer_knn__n_neighbors': [1, 2, 4, 8, 16, 32, 64],\n",
    "              'transformer__imputer_knn__weights': ['uniform', 'distance']\n",
    "              }\n",
    "\n",
    "model = aux.fit_model(X_train, y_train, transformer,\n",
    "                      LogisticRegression(), param_grid=param_grid)\n",
    "aux.predict_model(X_test, y_test, model)\n",
    "\n",
    "model.best_params_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after GridSearch there is no difference between `SimpleImputer` and `KNNImputer`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvements: Additional Features\n",
    "\n",
    "Let's add more features from the original dataset to see if the model improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pclass      0.000000\n",
      "Sex         0.000000\n",
      "SibSp       0.000000\n",
      "Parch       0.000000\n",
      "Age         0.198653\n",
      "Fare        0.000000\n",
      "Cabin       0.771044\n",
      "Embarked    0.002245\n",
      "dtype: float64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       105\n",
      "           1       0.78      0.73      0.76        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.79      0.80       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select features\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\n",
    "            \"Age\", \"Fare\", \"Cabin\", \"Embarked\"]\n",
    "num_features = ['Age', 'Fare']\n",
    "cat_features = [f for f in features if f not in num_features]\n",
    "\n",
    "# Check fraction of nulls\n",
    "print(train[features].isnull().mean())\n",
    "\n",
    "# Test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train[features], y, test_size=0.2, random_state=42)\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    ('ohe_features', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
    "    ('imputer_knn', KNNImputer(n_neighbors=1), num_features),\n",
    "])\n",
    "\n",
    "model = aux.fit_model(X_train, y_train, transformer,\n",
    "                      LogisticRegression())\n",
    "aux.predict_model(X_test, y_test, model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy still remains at 80% with some slight differences in precision and recall compared to the previous model.\n",
    "\n",
    "Let's try a few different estimators and see if there's any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       105\n",
      "           1       0.78      0.73      0.76        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.79      0.80       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "RidgeClassifierCV\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83       105\n",
      "           1       0.77      0.69      0.73        74\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.78      0.77      0.78       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       105\n",
      "           1       0.82      0.73      0.77        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.82      0.81      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.83       105\n",
      "           1       0.79      0.70      0.74        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.78      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       105\n",
      "           1       0.75      0.70      0.73        74\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.78      0.77      0.77       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n",
      "XGBClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.83       105\n",
      "           1       0.76      0.73      0.74        74\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.79      0.78      0.79       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimators = [LogisticRegression(),\n",
    "              RidgeClassifierCV(alphas=[1e-4, 1e-3, 1e-2, 1e-1]),\n",
    "              RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "              svm.SVC(kernel='linear', C=1, gamma='auto'),\n",
    "              svm.SVC(kernel='rbf', C=1, gamma='auto'),\n",
    "              XGBClassifier(n_estimators=50, random_state=42)\n",
    "              ]\n",
    "\n",
    "for estimator in estimators:\n",
    "    model = aux.fit_model(X_train, y_train, transformer,\n",
    "                          estimator)\n",
    "    print(estimator.__class__.__name__)\n",
    "    aux.predict_model(X_test, y_test, model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the estimators above appear to be significantly better than the others (before hyperparameter tuning). The accuracy of `RandomForestClassifier()` is marginally better so let's try GridSearch with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85       105\n",
      "           1       0.84      0.65      0.73        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.81      0.78      0.79       179\n",
      "weighted avg       0.81      0.80      0.80       179\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'est__max_depth': 8, 'est__min_samples_leaf': 1, 'est__min_samples_split': 2}"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(aux)\n",
    "\n",
    "transformer = ColumnTransformer([\n",
    "    ('ohe_features', OneHotEncoder(handle_unknown='ignore'), cat_features),\n",
    "    ('imputer_knn', KNNImputer(missing_values=np.nan), num_features),\n",
    "])\n",
    "\n",
    "param_grid = {'est__max_depth': [8, 16, 32, 64, 128],\n",
    "              'est__min_samples_leaf': [1, 2, 3, 4],\n",
    "              'est__min_samples_split': [2, 3, 4],\n",
    "              }\n",
    "\n",
    "model = aux.fit_model(X_train, y_train, transformer,\n",
    "                      RandomForestClassifier(\n",
    "                          n_estimators=100, random_state=42),\n",
    "                      param_grid=param_grid)\n",
    "aux.predict_model(X_test, y_test, model)\n",
    "\n",
    "model.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
